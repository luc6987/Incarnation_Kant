"""å‘é‡æ•°æ®åº“æ„å»ºæ¨¡å— - ä½¿ç”¨æœ¬åœ° GPU è¿›è¡Œ Embedding è®¡ç®—ã€‚

æœ¬æ¨¡å—ä½¿ç”¨ BAAI/bge-m3 æ¨¡å‹åœ¨æœ¬åœ° GPU (A4000) ä¸Šç”Ÿæˆå‘é‡åµŒå…¥ï¼Œ
å¹¶å°†æ–‡æœ¬å—å­˜å‚¨åˆ° ChromaDB å‘é‡æ•°æ®åº“ä¸­ã€‚

ç¼“å­˜ç›®å½•é…ç½®ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰:
    1. ç¯å¢ƒå˜é‡ HF_HOME æˆ– TRANSFORMERS_CACHEï¼ˆå¦‚æœå·²è®¾ç½®ï¼‰
    2. .venv/.cache/huggingfaceï¼ˆå¦‚æœ .venv ç›®å½•å­˜åœ¨ï¼‰
    3. ~/.cache/huggingfaceï¼ˆé»˜è®¤ä½ç½®ï¼‰
    
    è‡ªåŠ¨ä½¿ç”¨ .venv å¯ä»¥é¿å…ç”¨æˆ·ä¸»ç›®å½•çš„ç£ç›˜é…é¢é—®é¢˜ã€‚
    å¦‚æœé‡åˆ°ç£ç›˜é…é¢é—®é¢˜ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
    export HF_HOME=/path/to/larger/disk/.cache/huggingface
    export TRANSFORMERS_CACHE=/path/to/larger/disk/.cache/huggingface

æ¨¡å‹ä¿¡æ¯:
    - æ¨¡å‹åç§°: BAAI/bge-m3
    - æ¨¡å‹å¤§å°: ~2.3GB
    - å‘é‡ç»´åº¦: 1024
    - æ”¯æŒè¯­è¨€: å¤šè¯­è¨€ï¼ˆåŒ…æ‹¬å¾·è¯­å’Œè‹±è¯­ï¼‰
"""

import os
import json
import shutil
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional

# ä¿®å¤ SQLite ç‰ˆæœ¬é—®é¢˜ï¼šä½¿ç”¨ pysqlite3 æ›¿ä»£ç³»ç»Ÿ sqlite3
try:
    __import__("pysqlite3")
    sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")
except ImportError:
    pass  # å¦‚æœ pysqlite3 ä¸å¯ç”¨ï¼Œä½¿ç”¨ç³»ç»Ÿ sqlite3

# åœ¨å¯¼å…¥ HuggingFace ç›¸å…³åº“ä¹‹å‰è®¾ç½®ç¼“å­˜å’Œä¸´æ—¶ç›®å½•
# è¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰ä¸‹è½½å’Œä¸´æ—¶æ–‡ä»¶éƒ½å­˜å‚¨åœ¨ .venv ä¸­
def get_hf_cache_dir() -> str:
    """è·å– HuggingFace ç¼“å­˜ç›®å½•ã€‚
    
    ä¼˜å…ˆçº§ï¼š
    1. ç¯å¢ƒå˜é‡ HF_HOME æˆ– TRANSFORMERS_CACHE
    2. .venv/.cache/huggingfaceï¼ˆå¦‚æœ .venv å­˜åœ¨ï¼‰
    3. ~/.cache/huggingfaceï¼ˆé»˜è®¤ï¼‰
    
    Returns:
        ç¼“å­˜ç›®å½•è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
    """
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    cache_dir = os.getenv("HF_HOME") or os.getenv("TRANSFORMERS_CACHE")
    
    if not cache_dir:
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ .venv ç›®å½•
        venv_cache = Path(".venv/.cache/huggingface")
        if Path(".venv").exists():
            cache_dir = str(venv_cache.absolute())
            venv_cache.mkdir(parents=True, exist_ok=True)
        else:
            cache_dir = os.path.expanduser("~/.cache/huggingface")
    
    return cache_dir


def setup_hf_environment() -> None:
    """è®¾ç½® HuggingFace ç›¸å…³çš„ç¯å¢ƒå˜é‡ï¼Œå°†ç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶éƒ½å­˜å‚¨åˆ° .venv ä¸­ã€‚"""
    cache_dir = get_hf_cache_dir()
    
    # è®¾ç½® HuggingFace ç¼“å­˜ç›®å½•
    os.environ["HF_HOME"] = cache_dir
    os.environ["TRANSFORMERS_CACHE"] = cache_dir
    
    # è®¾ç½®ä¸´æ—¶ç›®å½•ï¼ˆHuggingFace ä¸‹è½½æ—¶ä¼šä½¿ç”¨ï¼‰
    # ä½¿ç”¨ .venv/.tmp ä½œä¸ºä¸´æ—¶ç›®å½•ï¼Œé¿å…ä½¿ç”¨ç”¨æˆ·ä¸»ç›®å½•çš„ä¸´æ—¶ç›®å½•
    if Path(".venv").exists():
        tmp_dir = str(Path(".venv/.tmp").absolute())
        Path(tmp_dir).mkdir(parents=True, exist_ok=True)
        os.environ["TMPDIR"] = tmp_dir
        os.environ["TMP"] = tmp_dir
        os.environ["TEMP"] = tmp_dir

# æå‰è°ƒç”¨ä»¥è®¾ç½®ç¯å¢ƒå˜é‡
setup_hf_environment()

import chromadb
from chromadb.config import Settings
from langchain_huggingface import HuggingFaceEmbeddings
from loguru import logger
import torch
from tqdm import tqdm


class KantVectorDB:
    """åº·å¾·æ–‡æœ¬å‘é‡æ•°æ®åº“ç±»ã€‚
    
    ä½¿ç”¨æœ¬åœ° GPU è¿è¡Œ BAAI/bge-m3 æ¨¡å‹è¿›è¡Œå‘é‡åµŒå…¥è®¡ç®—ï¼Œ
    å¹¶å°†ç»“æœå­˜å‚¨åˆ° ChromaDB æŒä¹…åŒ–æ•°æ®åº“ä¸­ã€‚
    """

    def __init__(
        self,
        persist_directory: str = "./data/chromadb",
        model_name: str = "BAAI/bge-m3",
        batch_size: int = 256,
    ) -> None:
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“ã€‚
        
        Args:
            persist_directory: ChromaDB æŒä¹…åŒ–å­˜å‚¨ç›®å½•
            model_name: HuggingFace æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º BAAI/bge-m3
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º 256ï¼ˆé’ˆå¯¹ A4000 16GB ä¼˜åŒ–ï¼‰
        
        Raises:
            RuntimeError: å½“ GPU ä¸å¯ç”¨ä½†å°è¯•ä½¿ç”¨ CUDA æ—¶
        """
        # 1. æ£€æŸ¥ GPU çŠ¶æ€
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ–¥ï¸  Hardware Check: Running on {device.upper()}")
        
        if device == "cuda":
            gpu_name = torch.cuda.get_device_name(0)
            vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"   GPU: {gpu_name}")
            logger.info(f"   VRAM: {vram_gb:.2f} GB")
        else:
            logger.warning("âš ï¸  No GPU detected, falling back to CPU (slower)")

        # 2. åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹ (BGE-M3)
        # ç¯å¢ƒå˜é‡å·²åœ¨æ¨¡å—å¯¼å…¥æ—¶è®¾ç½®ï¼Œè¿™é‡Œåªæ˜¾ç¤ºä¿¡æ¯
        cache_dir = os.getenv("HF_HOME") or os.getenv("TRANSFORMERS_CACHE") or get_hf_cache_dir()
        tmp_dir = os.getenv("TMPDIR") or os.getenv("TMP")
        
        # æ˜¾ç¤ºä½¿ç”¨çš„ç¼“å­˜å’Œä¸´æ—¶ç›®å½•
        logger.info(f"ğŸ“ HuggingFace cache: {cache_dir}")
        if tmp_dir and Path(".venv").exists() and tmp_dir == str(Path(".venv/.tmp").absolute()):
            logger.info(f"ğŸ“ Temporary files: {tmp_dir} (using .venv to avoid quota issues)")
        elif tmp_dir:
            logger.info(f"ğŸ“ Temporary files: {tmp_dir}")
        
        logger.info(f"ğŸ“¥ Loading local embedding model ({model_name})...")
        logger.info("   Note: Model size ~2.3GB. First download may take time.")
        
        try:
            self.embedding_fn = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={"device": device},
                encode_kwargs={"normalize_embeddings": True},  # å½’ä¸€åŒ–æœ‰åŠ©äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
            )
            logger.success(f"âœ… Model loaded successfully on {device}")
        except RuntimeError as e:
            error_msg = str(e)
            if "Disk quota exceeded" in error_msg or "os error 122" in error_msg:
                logger.error("âŒ Disk quota exceeded during model download!")
                logger.error("")
                logger.error("ğŸ’¡ Solutions:")
                logger.error("   1. Free up disk space (need ~3GB for model download)")
                logger.error("   2. Set custom cache directory with more space:")
                logger.error("      export HF_HOME=/path/to/larger/disk/.cache/huggingface")
                logger.error("      export TRANSFORMERS_CACHE=/path/to/larger/disk/.cache/huggingface")
                logger.error("   3. Or download model manually and place in cache directory")
                logger.error("")
                logger.error("   Model repository: https://huggingface.co/BAAI/bge-m3")
                raise RuntimeError(
                    "Disk quota exceeded. Please free up space or set HF_HOME/TRANSFORMERS_CACHE "
                    "to a directory with sufficient space."
                ) from e
            else:
                logger.error(f"âŒ Failed to load embedding model: {e}")
                raise
        except Exception as e:
            logger.error(f"âŒ Failed to load embedding model: {e}")
            logger.error("")
            logger.error("ğŸ’¡ Troubleshooting:")
            logger.error("   - Check internet connection for model download")
            logger.error("   - Verify disk space availability (~3GB needed)")
            logger.error("   - Check HuggingFace access (may need login)")
            raise

        # 3. åˆå§‹åŒ– ChromaDB
        # æ³¨æ„ï¼šBGE-M3 çš„ç»´åº¦æ˜¯ 1024 (Dense)ï¼Œä¸åŒäº OpenAI çš„ 1536/3072
        persist_path = Path(persist_directory)
        persist_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ“¦ Initializing ChromaDB at {persist_directory}...")
        self.client = chromadb.PersistentClient(
            path=str(persist_path),
            settings=Settings(anonymized_telemetry=False),
        )
        
        self.collection = self.client.get_or_create_collection(
            name="kant_corpus_local",
            metadata={"hnsw:space": "cosine", "model": model_name},
        )
        logger.info(f"âœ… ChromaDB collection 'kant_corpus_local' ready")
        
        self.batch_size = batch_size

    def _flatten_metadata(self, meta: Dict[str, Any]) -> Dict[str, Any]:
        """å°†å…ƒæ•°æ®æ‰å¹³åŒ–ï¼Œå¤„ç† list ç±»å‹ã€‚
        
        ChromaDB çš„ metadata å­—æ®µä¸æ”¯æŒ list ç±»å‹ï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚
        
        Args:
            meta: åŸå§‹å…ƒæ•°æ®å­—å…¸
        
        Returns:
            æ‰å¹³åŒ–åçš„å…ƒæ•°æ®å­—å…¸
        """
        clean_meta: Dict[str, Any] = {}
        for k, v in meta.items():
            if isinstance(v, list):
                clean_meta[k] = ", ".join(map(str, v))
            elif v is None:
                clean_meta[k] = ""
            else:
                clean_meta[k] = str(v)
        return clean_meta

    def ingest_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """æ‰¹é‡å†™å…¥æ–‡æœ¬å—åˆ°å‘é‡æ•°æ®åº“ã€‚
        
        ä½¿ç”¨ GPU æ‰¹å¤„ç†è¿›è¡Œå‘é‡åµŒå…¥è®¡ç®—ï¼Œæé«˜æ•ˆç‡ã€‚
        
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'id', 'content', 'metadata' å­—æ®µ
        
        Raises:
            ValueError: å½“ chunks ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®æ—¶
        """
        if not chunks:
            logger.warning("âš ï¸  No chunks to ingest")
            return

        logger.info(f"ğŸš€ Starting LOCAL ingestion of {len(chunks)} chunks...")
        logger.info(f"   Batch size: {self.batch_size}")

        batch_ids: List[str] = []
        batch_documents: List[str] = []
        batch_metadatas: List[Dict[str, Any]] = []

        total_batches = (len(chunks) + self.batch_size - 1) // self.batch_size
        processed = 0

        with tqdm(total=len(chunks), desc="Embedding on GPU") as pbar:
            for chunk in chunks:
                # éªŒè¯ chunk æ ¼å¼
                if not all(key in chunk for key in ["id", "content", "metadata"]):
                    logger.warning(f"âš ï¸  Skipping invalid chunk: missing required fields")
                    continue

                batch_ids.append(chunk["id"])
                batch_documents.append(chunk["content"])
                batch_metadatas.append(self._flatten_metadata(chunk["metadata"]))

                # è¾¾åˆ°æ‰¹å¤„ç†å¤§å°æ—¶ï¼Œæ‰§è¡ŒåµŒå…¥å’Œå†™å…¥
                if len(batch_ids) >= self.batch_size:
                    self._upsert_batch(batch_ids, batch_documents, batch_metadatas)
                    processed += len(batch_ids)
                    pbar.update(len(batch_ids))
                    
                    batch_ids = []
                    batch_documents = []
                    batch_metadatas = []

            # å¤„ç†å‰©ä½™çš„æ•°æ®
            if batch_ids:
                self._upsert_batch(batch_ids, batch_documents, batch_metadatas)
                processed += len(batch_ids)
                pbar.update(len(batch_ids))

        final_count = self.collection.count()
        logger.success(
            f"âœ… Ingestion complete. Processed {processed} chunks. "
            f"Collection count: {final_count}"
        )

    def _upsert_batch(
        self,
        ids: List[str],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ) -> None:
        """æ‰§è¡Œå•æ‰¹æ•°æ®çš„åµŒå…¥å’Œå†™å…¥ã€‚
        
        Args:
            ids: æ–‡æ¡£ ID åˆ—è¡¨
            documents: æ–‡æ¡£å†…å®¹åˆ—è¡¨
            metadatas: å…ƒæ•°æ®åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è®¡ç®—å‘é‡
            embeddings = self.embedding_fn.embed_documents(documents)
            
            # å†™å…¥ ChromaDB
            self.collection.upsert(
                ids=ids,
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas,
            )
        except Exception as e:
            logger.error(f"âŒ Failed to upsert batch: {e}")
            raise


def load_chunks_from_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """ä» JSONL æ–‡ä»¶åŠ è½½æ–‡æœ¬å—ã€‚
    
    Args:
        file_path: JSONL æ–‡ä»¶è·¯å¾„
    
    Returns:
        æ–‡æœ¬å—åˆ—è¡¨
    
    Raises:
        FileNotFoundError: å½“æ–‡ä»¶ä¸å­˜åœ¨æ—¶
        json.JSONDecodeError: å½“ JSON è§£æå¤±è´¥æ—¶
    """
    file_path_obj = Path(file_path)
    if not file_path_obj.exists():
        raise FileNotFoundError(f"File not found: {file_path}")

    chunks: List[Dict[str, Any]] = []
    logger.info(f"ğŸ“‚ Loading chunks from {file_path}...")
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    chunk = json.loads(line)
                    chunks.append(chunk)
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸  Failed to parse line {line_num} in {file_path}: {e}")
                    continue
        
        logger.info(f"âœ… Loaded {len(chunks)} chunks from {file_path}")
        return chunks
    except Exception as e:
        logger.error(f"âŒ Error reading {file_path}: {e}")
        raise


def find_all_chunk_files(chunks_dir: str = "data/chunks") -> List[str]:
    """æŸ¥æ‰¾æ‰€æœ‰ JSONL æ ¼å¼çš„ chunk æ–‡ä»¶ã€‚
    
    Args:
        chunks_dir: chunks ç›®å½•è·¯å¾„
    
    Returns:
        æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ŒæŒ‰æ–‡ä»¶åæ’åº
    """
    chunks_path = Path(chunks_dir)
    if not chunks_path.exists():
        logger.warning(f"âš ï¸  Chunks directory not found: {chunks_dir}")
        return []

    jsonl_files = sorted(chunks_path.glob("*.jsonl"))
    file_paths = [str(f) for f in jsonl_files]
    
    logger.info(f"ğŸ“ Found {len(file_paths)} JSONL files in {chunks_dir}")
    return file_paths


def check_disk_space(path: str, required_gb: float = 3.0) -> bool:
    """æ£€æŸ¥æŒ‡å®šè·¯å¾„çš„ç£ç›˜ç©ºé—´æ˜¯å¦è¶³å¤Ÿã€‚
    
    Args:
        path: è¦æ£€æŸ¥çš„è·¯å¾„
        required_gb: éœ€è¦çš„ç©ºé—´ï¼ˆGBï¼‰
    
    Returns:
        å¦‚æœç©ºé—´è¶³å¤Ÿè¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    try:
        stat = shutil.disk_usage(path)
        free_gb = stat.free / (1024**3)
        logger.info(f"ğŸ’¾ Disk space check for {path}:")
        logger.info(f"   Free: {free_gb:.2f} GB, Required: {required_gb:.2f} GB")
        return free_gb >= required_gb
    except Exception as e:
        logger.warning(f"âš ï¸  Could not check disk space: {e}")
        return True  # å‡è®¾ç©ºé—´è¶³å¤Ÿï¼Œç»§ç»­æ‰§è¡Œ


def main() -> None:
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå‘é‡æ•°æ®åº“æ„å»ºæµç¨‹ã€‚"""
    # é…ç½®
    DB_PATH = "./data/chromadb"
    CHUNKS_DIR = "data/chunks"
    
    # ç¡®å®šç¼“å­˜ç›®å½•
    cache_dir = get_hf_cache_dir()
    
    logger.info("ğŸ“‹ Configuration:")
    logger.info(f"   ChromaDB path: {DB_PATH}")
    logger.info(f"   HuggingFace cache: {cache_dir}")
    if Path(".venv").exists() and cache_dir == str(Path(".venv/.cache/huggingface").absolute()):
        logger.info("   âœ… Using .venv for model storage (avoids quota issues)")
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    if not check_disk_space(cache_dir, required_gb=3.0):
        logger.warning("âš ï¸  Insufficient disk space in cache directory!")
        logger.warning("   Set HF_HOME or TRANSFORMERS_CACHE to a directory with more space.")
        logger.warning("   Example: export HF_HOME=/path/to/larger/disk/.cache/huggingface")
    
    # æ£€æŸ¥å¹¶æ¸…ç†æ—§æ•°æ®åº“ï¼ˆå¦‚æœå­˜åœ¨ä¸”ç»´åº¦ä¸åŒ¹é…ï¼‰
    if os.path.exists(DB_PATH):
        logger.warning(f"âš ï¸  Found existing DB at {DB_PATH}")
        logger.info("   Deleting old database (BGE-M3 uses 1024 dimensions, incompatible with OpenAI embeddings)")
        try:
            shutil.rmtree(DB_PATH)
            logger.info("ğŸ—‘ï¸  Deleted old database.")
        except Exception as e:
            logger.error(f"âŒ Failed to delete old database: {e}")
            raise

    # æŸ¥æ‰¾æ‰€æœ‰ chunk æ–‡ä»¶
    chunk_files = find_all_chunk_files(CHUNKS_DIR)
    if not chunk_files:
        logger.error(f"âŒ No chunk files found in {CHUNKS_DIR}")
        return

    # åŠ è½½æ‰€æœ‰ chunks
    all_chunks: List[Dict[str, Any]] = []
    for chunk_file in chunk_files:
        try:
            chunks = load_chunks_from_jsonl(chunk_file)
            all_chunks.extend(chunks)
        except Exception as e:
            logger.error(f"âŒ Failed to load {chunk_file}: {e}")
            continue

    if not all_chunks:
        logger.error("âŒ No chunks loaded. Exiting.")
        return

    logger.info(f"ğŸ“Š Total chunks to ingest: {len(all_chunks)}")

    # åˆå§‹åŒ–æ•°æ®åº“å¹¶æ‰§è¡Œå†™å…¥
    try:
        db = KantVectorDB(persist_directory=DB_PATH, batch_size=256)
        db.ingest_chunks(all_chunks)
        logger.success("ğŸ‰ Vector database construction completed successfully!")
    except Exception as e:
        logger.error(f"âŒ Failed to build vector database: {e}")
        raise


if __name__ == "__main__":
    main()
